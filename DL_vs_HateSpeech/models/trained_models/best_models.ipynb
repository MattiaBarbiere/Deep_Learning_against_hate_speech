{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fbd1d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattb\\anaconda3\\envs\\DLHS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Our imports\n",
    "from DL_vs_HateSpeech.loading_data.dataloader import DataLoader\n",
    "from DL_vs_HateSpeech.training.training import collate_fn\n",
    "from DL_vs_HateSpeech.utils import check_frozen_params\n",
    "from DL_vs_HateSpeech.models.utils import load_model_from_path\n",
    "from DL_vs_HateSpeech.evaluation.evaluate import evaluate\n",
    "\n",
    "\n",
    "# Some constants\n",
    "DATA_SUBSET = \"us_pol\"\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# Load Data\n",
    "train_dataset = DataLoader(type=\"train\", subset=DATA_SUBSET)\n",
    "test_dataset = DataLoader(type=\"test\", subset=DATA_SUBSET)\n",
    "train_loader = TorchDataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = TorchDataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0624dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate all the models\n",
    "def create_metric_df(path):\n",
    "    \"\"\"\n",
    "    Create a DataFrame with the metrics of all models in the given path.\n",
    "    \"\"\"\n",
    "    # Get all the model files in the directory\n",
    "    model_files = [f for f in os.listdir(path) if f.endswith('.pth')]\n",
    "    \n",
    "    # Dataset to store the model accuracies and F1 scores\n",
    "    df = pd.DataFrame(columns=[\"accuracy\", \"f1_score_0\", \"f1_score_1\", \"avg_loss\"])\n",
    "    \n",
    "    for file_name in model_files:\n",
    "        model_v2_16 = load_model_from_path(path, file_name=file_name, device=\"cpu\")\n",
    "        model_v2_16.eval()\n",
    "        \n",
    "        # Check how many parameters are frozen\n",
    "        check_frozen_params(model_v2_16)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        avg_loss_test, accuracy_test, f1_test = evaluate(model_v2_16, test_loader, nn.BCEWithLogitsLoss(), device=\"cpu\")\n",
    "        \n",
    "        # Print the results\n",
    "        print(f\"Model: {file_name}\")\n",
    "        print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "        print(f\"F1 Score (0): {f1_test[0]:.4f}\")\n",
    "        print(f\"F1 Score (1): {f1_test[1]:.4f}\")\n",
    "        print(f\"Average Loss: {avg_loss_test:.4f}\")\n",
    "        \n",
    "        # Append the results to the DataFrame\n",
    "        df.loc[len(df)] = {\n",
    "            \"accuracy\": accuracy_test,\n",
    "            \"f1_score_0\": f1_test[0],\n",
    "            \"f1_score_1\": f1_test[1],\n",
    "            \"avg_loss\": avg_loss_test\n",
    "        }\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(os.path.join(path, \"model_metrics.csv\"), index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to compute average and standard deviation of metrics\n",
    "def compute_average_metrics(df):\n",
    "    \"\"\"\n",
    "    Compute the average and standard deviation of the metrics in the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Averge the F1 scores into a single metric\n",
    "    df['f1_score_avg'] = (df['f1_score_0'] + df['f1_score_1']) / 2\n",
    "    df = df.drop(columns=['f1_score_0', 'f1_score_1'])\n",
    "    df = df.rename(columns={'f1_score_avg': 'f1_score'})\n",
    "\n",
    "    # Compute average and standard deviation for each metric\n",
    "    avg_metrics = df.mean()\n",
    "    std_metrics = df.std()\n",
    "\n",
    "    # Print avg +_ std\n",
    "    print(\"\\nAverage Metrics with Standard Deviation:\")\n",
    "    for metric in avg_metrics.index:\n",
    "        print(f\"{metric}: {avg_metrics[metric] * 100:.2f} ± {std_metrics[metric] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e002ef3",
   "metadata": {},
   "source": [
    "# Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796f9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to true if the models need to be evaluated\n",
    "# Note: This will take a while to run\n",
    "EVALUATE_MODELS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee553aea",
   "metadata": {},
   "source": [
    "## Best model using CLIP of type 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500f6cc",
   "metadata": {},
   "source": [
    "### With augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241084cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ModelV2_clip_16_aug_True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9de03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_MODELS:\n",
    "    create_metric_df(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d63d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metrics with Standard Deviation:\n",
      "accuracy: 50.42 ± 4.42\n",
      "avg_loss: 69.33 ± 0.10\n",
      "f1_score: 38.22 ± 10.03\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(path, \"model_metrics.csv\"))\n",
    "compute_average_metrics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a00142",
   "metadata": {},
   "source": [
    "### Without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ce4078",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ModelV2_clip_16_aug_False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d126ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_MODELS:\n",
    "    create_metric_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83fef1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metrics with Standard Deviation:\n",
      "accuracy: 60.73 ± 2.00\n",
      "avg_loss: 161.80 ± 69.68\n",
      "f1_score: 59.50 ± 2.76\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(path, \"model_metrics.csv\"))\n",
    "compute_average_metrics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56ea26",
   "metadata": {},
   "source": [
    "## Best model using CLIP of type 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68b874",
   "metadata": {},
   "source": [
    "### With augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad344fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ModelV2_clip_32_aug_True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7609a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_MODELS:\n",
    "    create_metric_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102c84d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metrics with Standard Deviation:\n",
      "accuracy: 48.79 ± 0.92\n",
      "avg_loss: 69.33 ± 0.02\n",
      "f1_score: 35.87 ± 4.77\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(path, \"model_metrics.csv\"))\n",
    "compute_average_metrics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2c9df",
   "metadata": {},
   "source": [
    "### Without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29844555",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ModelV2_clip_32_aug_False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5442e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_MODELS:\n",
    "    create_metric_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "081ac56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metrics with Standard Deviation:\n",
      "accuracy: 57.18 ± 1.70\n",
      "avg_loss: 144.78 ± 57.45\n",
      "f1_score: 57.04 ± 1.77\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(path, \"model_metrics.csv\"))\n",
    "compute_average_metrics(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLHS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
